{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_Simple_Neural_Network_Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMo+YNcbCgDneU96UC6IDzC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravjain14/All-about-JAX/blob/main/Training_Simple_Neural_Network_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This is the third notebook in our series of \"All-about JAX\".\n",
        "\n",
        "In this notebook, we'll be combining what we learnt in [jax101.ipynb](https://github.com/gauravjain14/All-about-JAX/blob/main/jax101.ipynb) and [Flax_Basics.ipynb](https://github.com/gauravjain14/All-about-JAX/blob/main/Flax_Basics.ipynb) and use them to train a simple neural network with our beloved tensorflow/datasets Data Loading.\n",
        "\n",
        "This notebook follows the JAX tutorial [Training a Simple Neural Network, with tensorflow/datasets Data Loading](https://jax.readthedocs.io/en/latest/notebooks/neural_network_with_tfds_data.html)."
      ],
      "metadata": {
        "id": "_Oxa36fSvOO1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PkLJ5cohvAPC"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Getting a few bookkeeping items out of the way, for once and hopefully forever."
      ],
      "metadata": {
        "id": "MsrveJvqvE39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize weights and biases for a dense neural network layer\n",
        "def random_layer_params(m, n, key, scale=1e-2):\n",
        "  w_key, b_key = random.split(key)\n",
        "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
        "\n",
        "## Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
        "def init_network_params(sizes, key):\n",
        "  keys = random.split(key, len(sizes))\n",
        "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "layer_sizes = [784, 512, 512, 10]\n",
        "step_size = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "n_targets = 10\n",
        "params = init_network_params(layer_sizes, random.PRNGKey(0))\n",
        "for p in params:\n",
        "  print(p[0].shape, p[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZR4YgtBowq35",
        "outputId": "bb518e98-f4fb-4e72-e471-95e700bec287"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(512, 784) (512,)\n",
            "(512, 512) (512,)\n",
            "(10, 512) (10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auto-batching predictions\n",
        "\n",
        "Using JAX's `vmap` function here. The idea behind this is that we can use the following prediction function, defined for a single image example, for handling mini-batches without paying any **performance penalty**"
      ],
      "metadata": {
        "id": "SMIQ4pkBxG3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "def relu(x):\n",
        "  # We are defining relu here. I'm sure FLAX should already have one.\n",
        "  return jnp.maximum(0, x)\n",
        "\n",
        "def predict(params, image):\n",
        "  # per-example predictions\n",
        "  activations = image\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = jnp.dot(w, activations) + b\n",
        "    activations = relu(outputs)\n",
        "\n",
        "  final_w, final_b = params[-1]\n",
        "  logits = jnp.dot(final_w, activations) + final_b\n",
        "  return logits - logsumexp(logits)"
      ],
      "metadata": {
        "id": "ns0WNqMH1STb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This works on single examples\n",
        "random_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\n",
        "preds = predict(params, random_flattened_image)\n",
        "print(preds.shape)"
      ],
      "metadata": {
        "id": "-i8wJQGV1XKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c09a3c-6244-487f-c9b8-cb090be49199"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see if this works for multiple batches\n",
        "random_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\n",
        "try:\n",
        "  preds = predict(params, random_flattened_images)\n",
        "except TypeError:\n",
        "  print('Invalid shapes!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGNQD9IYlxEe",
        "outputId": "8b164902-53ff-41c8-b09f-7b522851226e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid shapes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to make batches work?\n",
        "\n",
        "We saw that our `predict` function didn't directly work when we tried to feed **batched** inputs. To change that, we use `jax.vmap` to map the `predict` function such that it can work with batched inputs"
      ],
      "metadata": {
        "id": "EwmEywIbCldm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_predict = vmap(predict, in_axes=(None, 0))\n",
        "\n",
        "batched_preds = batched_predict(params, random_flattened_images)\n",
        "print(batched_preds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRsORj-uImZC",
        "outputId": "8a37fdeb-11f5-42b1-f29e-19a316bcfc9c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AgEY0L5iYGe0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility and Loss functions"
      ],
      "metadata": {
        "id": "joLZpJsNJ5m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(x, k, dtype=jnp.float32):\n",
        "  \"\"\" Create a one-hot encoding of x of size k. \"\"\"\n",
        "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
        "\n",
        "def accuracy(params, images, targets):\n",
        "  target_class = jnp.argmax(targets, axis=1)\n",
        "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
        "  return jnp.mean(predicted_class == target_class)\n",
        "\n",
        "def loss(params, images, targets):\n",
        "  preds = batched_predict(params, images)\n",
        "  return -jnp.mean(preds * targets)\n",
        "\n",
        "@jit\n",
        "def update(params, x, y):\n",
        "  grads = grad(loss)(params, x, y)\n",
        "  return [(w - step_size * dw, b - step_size * db)\n",
        "            for (w, b), (dw, db) in zip(params, grads)]"
      ],
      "metadata": {
        "id": "x7fTN3YrYDnl"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MPFYziU7YrBR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading with tensorflow/datasets\n",
        "\n",
        "JAX finally decided to not reinvent the wheel here and rather choose one of the many amazing data loading frameworks available, viz a viz., the `tensorflow/datasets` data loader"
      ],
      "metadata": {
        "id": "iPtzIttzceUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Ensure TF does not see GPU and grab all the GPU memory\n",
        "tf.config.set_visible_devices([], device_type='GPU')\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "data_dir = '/tmp/tfds'\n",
        "\n",
        "## Fetch full datasets for evaluation\n",
        "# tfds.load return tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with\n",
        "# tfds.dataset_as_numpy\n",
        "mnist_data, info = tfds.load(name='mnist', batch_size=-1, data_dir=data_dir, \n",
        "                             with_info=True)\n",
        "mnist_data = tfds.as_numpy(mnist_data)\n",
        "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
        "num_labels = info.features['label'].num_classes\n",
        "h, w, c = info.features['image'].shape\n",
        "num_pixels = h * w * c"
      ],
      "metadata": {
        "id": "5RnC1OLscjTA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(info)"
      ],
      "metadata": {
        "id": "31eVjJrTeydk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e692bc29-23a2-46fc-f5cb-c5513385722f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='mnist',\n",
            "    full_name='mnist/3.0.1',\n",
            "    description=\"\"\"\n",
            "    The MNIST database of handwritten digits.\n",
            "    \"\"\",\n",
            "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
            "    data_path='/tmp/tfds/mnist/3.0.1',\n",
            "    file_format=tfrecord,\n",
            "    download_size=11.06 MiB,\n",
            "    dataset_size=21.00 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
            "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
            "    },\n",
            "    citation=\"\"\"@article{lecun2010mnist,\n",
            "      title={MNIST handwritten digit database},\n",
            "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
            "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
            "      volume={2},\n",
            "      year={2010}\n",
            "    }\"\"\",\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Extract images and labels from the dataset"
      ],
      "metadata": {
        "id": "mVb_590rxm_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Full train set\n",
        "train_images, train_labels = train_data['image'], train_data['label']\n",
        "train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
        "train_labels = one_hot(train_labels, num_labels)\n",
        "\n",
        "## Full test set\n",
        "test_images, test_labels = test_data['image'], test_data['label']\n",
        "test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
        "test_labels = one_hot(test_labels, num_labels)"
      ],
      "metadata": {
        "id": "3RqF0wDEyRn9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train: ', train_images.shape, train_labels.shape)\n",
        "print('Test: ', test_images.shape, test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-4aqHzzzSHY",
        "outputId": "b8a151b6-8d69-4157-accd-2a7bf90564a7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train:  (60000, 784) (60000, 10)\n",
            "Test:  (10000, 784) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2rScAvL3zWYW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop - it's here, Alas!\n",
        "\n",
        "A lot of the code in the following section is self-explanatory. I'll include comments where required and deemed necessary"
      ],
      "metadata": {
        "id": "y45RY0fr3R8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def get_train_batches():\n",
        "  \"\"\" This functions look sort of redundant due to all the cells we executed\n",
        "  in the Data Loading with tensorflow/datasets subsection.\n",
        "  \"\"\"\n",
        "  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
        "  ds = tfds.load(name='mnist', split='train', as_supervised=True, \\\n",
        "                 data_dir=data_dir)\n",
        "  ds = ds.batch(batch_size).prefetch(1)\n",
        "  # tfds.dataset_as_numpyu converts the tf.data.Dataset into an iterable of\n",
        "  # NumPy arrays\n",
        "  return tfds.as_numpy(ds)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "  for x, y in get_train_batches():\n",
        "    x = jnp.reshape(x, (len(x), num_pixels))\n",
        "    y = one_hot(y, num_labels)\n",
        "    params = update(params, x, y)\n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        " \n",
        "  train_acc = accuracy(params, train_images, train_labels)\n",
        "  test_acc = accuracy(params, test_images, test_labels)\n",
        "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "  print(\"Training set accuracy {}\".format(train_acc))\n",
        "  print(\"Test set accuracy {}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKSh0EKg3YkV",
        "outputId": "32a52030-22b1-403c-e5f1-5617651667b7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 in 20.61 sec\n",
            "Training set accuracy 0.9821500182151794\n",
            "Test set accuracy 0.9704999923706055\n",
            "Epoch 1 in 10.33 sec\n",
            "Training set accuracy 0.9837166666984558\n",
            "Test set accuracy 0.9714999794960022\n",
            "Epoch 2 in 20.58 sec\n",
            "Training set accuracy 0.9849166870117188\n",
            "Test set accuracy 0.972000002861023\n",
            "Epoch 3 in 10.33 sec\n",
            "Training set accuracy 0.9860166907310486\n",
            "Test set accuracy 0.9731999635696411\n",
            "Epoch 4 in 10.68 sec\n",
            "Training set accuracy 0.9870833158493042\n",
            "Test set accuracy 0.9739999771118164\n",
            "Epoch 5 in 20.57 sec\n",
            "Training set accuracy 0.9879500269889832\n",
            "Test set accuracy 0.9747999906539917\n",
            "Epoch 6 in 13.16 sec\n",
            "Training set accuracy 0.9890666604042053\n",
            "Test set accuracy 0.9751999974250793\n",
            "Epoch 7 in 20.61 sec\n",
            "Training set accuracy 0.9900833368301392\n",
            "Test set accuracy 0.9752999544143677\n",
            "Epoch 8 in 9.26 sec\n",
            "Training set accuracy 0.9908000230789185\n",
            "Test set accuracy 0.9756999611854553\n",
            "Epoch 9 in 10.33 sec\n",
            "Training set accuracy 0.9916000366210938\n",
            "Test set accuracy 0.9763999581336975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l46iwRo73bjz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One additional experiment\n",
        "\n",
        "In the training loop above, we have manually defined a loss function. However, to quickly understand the impact of loss function on the overall training efficiency, I am just playing around with one more pre-defined loss function ([by jaxopt](https://jaxopt.github.io/stable/objective_and_loss.html))"
      ],
      "metadata": {
        "id": "E0l9UIPjAI0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jaxopt\n",
        "import jaxopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUoYok80Ad5O",
        "outputId": "a2fc7add-983e-48c8-88d6-df122b81261c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jaxopt in /usr/local/lib/python3.7/dist-packages (0.4.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from jaxopt) (1.2.0)\n",
            "Requirement already satisfied: jax>=0.2.18 in /usr/local/lib/python3.7/dist-packages (from jaxopt) (0.3.14)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jaxopt) (1.7.3)\n",
            "Requirement already satisfied: matplotlib>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from jaxopt) (3.2.2)\n",
            "Requirement already satisfied: numpy<1.23.0 in /usr/local/lib/python3.7/dist-packages (from jaxopt) (1.21.6)\n",
            "Requirement already satisfied: jaxlib>=0.1.69 in /usr/local/lib/python3.7/dist-packages (from jaxopt) (0.3.14+cuda11.cudnn805)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->jaxopt) (3.3.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->jaxopt) (0.6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.2.18->jaxopt) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.69->jaxopt) (2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.1->jaxopt) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.1->jaxopt) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.1->jaxopt) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.0.1->jaxopt) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.0.1->jaxopt) (1.15.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.2.18->jaxopt) (3.8.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.2.18->jaxopt) (5.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# redefine the `update` function from above\n",
        "\n",
        "def update(params, x, y):\n",
        "  print(y.shape)\n",
        "  print(batched_predict(params, x).shape)\n",
        "  grads = grad(jaxopt.loss.huber_loss)(y, batched_predict(params, x))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "  for x, y in get_train_batches():\n",
        "    x = jnp.reshape(x, (len(x), num_pixels))\n",
        "    y = one_hot(y, num_labels)\n",
        "    params = update(params, x, y)\n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  train_acc = accuracy(params, train_images, train_labels)\n",
        "  test_acc = accuracy(params, test_images, test_labels)\n",
        "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "  print(\"Training set accuracy {}\".format(train_acc))\n",
        "  print(\"Test set accuracy {}\".format(test_acc))"
      ],
      "metadata": {
        "id": "0etFHmFWAg90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0oNBL3SKCGRR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}