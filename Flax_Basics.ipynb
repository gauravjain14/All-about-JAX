{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flax Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPeir4KIODK4qRxqL6nPt1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravjain14/All-about-JAX/blob/main/Flax_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q pip jax jaxlib \n",
        "!pip install --upgrade git+https://github.com/google/flax.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auHDS0OkneXE",
        "outputId": "d89d96d5-0d65-4c7e-fff4-4ef40d4b2ed2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 42.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 72.0 MB 211 kB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/google/flax.git\n",
            "  Cloning https://github.com/google/flax.git to /tmp/pip-req-build-bpiwk30f\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google/flax.git /tmp/pip-req-build-bpiwk30f\n",
            "  Resolved https://github.com/google/flax.git to commit 6ae22681ef6f6c004140c3759e7175533bda55bd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (1.21.6)\n",
            "Requirement already satisfied: jax>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (0.3.15)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (1.0.4)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (0.1.3)\n",
            "Requirement already satisfied: rich~=11.1 in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (11.2.0)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (0.1.21)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (4.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.2->flax==0.5.3) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.2->flax==0.5.3) (1.7.3)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.2->flax==0.5.3) (0.6.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.2->flax==0.5.3) (3.3.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich~=11.1->flax==0.5.3) (2.6.1)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from rich~=11.1->flax==0.5.3) (0.4.5)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich~=11.1->flax==0.5.3) (0.9.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.5.3) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.5.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.5.3) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.5.3) (1.4.4)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax->flax==0.5.3) (0.3.15)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax==0.5.3) (0.1.3)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax==0.5.3) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax==0.5.3) (0.1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->flax==0.5.3) (1.15.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.3.2->flax==0.5.3) (3.8.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.3.2->flax==0.5.3) (5.9.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YQG3pqpPeVPL"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from typing import Any, Callable, Sequence\n",
        "from jax import lax, random, numpy as jnp\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression with Flax "
      ],
      "metadata": {
        "id": "pYIvTWQQnT_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create one dense layer instance\n",
        "# We only specify the output size of the model. Size of the \n",
        "# input is idenfied by the correct size of the kerne\n",
        "model = nn.Dense(features=5) # Number of 'features' parameter as input "
      ],
      "metadata": {
        "id": "pJ2-Tn2hqRgW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Follow this up with Model parameters and Initialization"
      ],
      "metadata": {
        "id": "33wFesCGqnDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key1, key2 = random.split(random.PRNGKey(0))\n",
        "x = random.normal(key1, (10,))  # generate dummy input\n",
        "params = model.init(key2, x)\n",
        "jax.tree_util.tree_map(lambda x: x.shape, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EKETZ0Rq7F7",
        "outputId": "ce5928f8-c9b8-4544-d0b8-d6e2cab560ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    params: {\n",
              "        bias: (5,),\n",
              "        kernel: (10, 5),\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The params created are immutable i.e. retains the functional nature of JAX\n",
        "try:\n",
        "  params['new_key'] = jnp.ones((2,2))\n",
        "except ValueError as e:\n",
        "  print(\"Error: \", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am31haIYrWOV",
        "outputId": "b52c6035-c96c-4dcc-c7c3-c1450360c0f0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error:  FrozenDict is immutable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How do we evaluate a model given a set of parameters?\n",
        "\n",
        "We execute `model.apply(parameters, input)`\n",
        "\n",
        "Note: Seems like when we call the `print` method on the model output, it copies that to the host."
      ],
      "metadata": {
        "id": "2tVEV9CZtyZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = model.apply(params, x)\n",
        "model_output\n",
        "# print(model_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI9_N9TYuQ3G",
        "outputId": "9d3339c2-9ab4-4b3d-993b-40b7ff8b5968"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([-1.3721193 ,  0.61131495,  0.6442836 ,  2.2192965 ,\n",
              "             -1.1271116 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mf0VbsuauZ7k"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Now that we know how to initialize parameters and apply them to a model, let's learn how to use the same \"immutable\" parameters and execute **Gradient Descent**. \n",
        "\n",
        "Confused about **Gradient Descent**, feel free to take a look at [this link](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#gradient-descent)"
      ],
      "metadata": {
        "id": "QEUHF9CBwtDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's begin with setting up initial probelm dimensions\n",
        "n_samples = 20\n",
        "x_dim = 10\n",
        "y_dim = 5\n",
        "\n",
        "## Initialize parameters (W) and bias (b)\n",
        "key = random.PRNGKey(0)\n",
        "k1, k2 = random.split(key)\n",
        "W = random.normal(k1, (x_dim, y_dim))\n",
        "b = random.normal(k2, (y_dim,))\n",
        "\n",
        "# This is a first - store the parameters in a pytree\n",
        "true_params = freeze({'params': {'bias':b, 'kernel': W}})\n",
        "# We can look at how this pytree looks like\n",
        "true_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK3LJVIXxKgH",
        "outputId": "f2e3639f-a5d2-45f3-9990-11be0585546e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    params: {\n",
              "        bias: DeviceArray([-1.4581939, -2.047044 ,  2.0473392,  1.1684095, -0.9758364],            dtype=float32),\n",
              "        kernel: DeviceArray([[ 1.0247566 ,  0.18528093,  0.03387944, -0.86629736,\n",
              "                       0.34718114],\n",
              "                     [ 1.7656006 ,  0.99169755,  1.1657897 ,  1.1106981 ,\n",
              "                      -0.08589564],\n",
              "                     [-1.1820309 ,  0.29050717,  1.436301  ,  0.15073189,\n",
              "                      -1.3651401 ],\n",
              "                     [-1.1463748 , -0.16064964,  0.04578291,  1.3267074 ,\n",
              "                       0.08830649],\n",
              "                     [ 0.15840754,  1.3908992 , -1.3764939 ,  0.4419787 ,\n",
              "                      -2.2242246 ],\n",
              "                     [ 0.5943986 ,  0.8191525 ,  0.32800463,  0.51409715,\n",
              "                       0.92392564],\n",
              "                     [-0.32272884,  1.7835051 ,  1.0902369 , -0.5799917 ,\n",
              "                       0.9487662 ],\n",
              "                     [ 0.97157586, -1.2998172 ,  0.3205269 ,  0.806568  ,\n",
              "                      -1.1939563 ],\n",
              "                     [ 1.001932  , -0.6774013 ,  1.0407888 , -1.8285896 ,\n",
              "                      -0.4360311 ],\n",
              "                     [-0.62268263,  0.47034723, -1.1581832 , -0.71849173,\n",
              "                       0.13199319]], dtype=float32),\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate input samples with additional noise\n",
        "key_sample, key_noise = random.split(k1)\n",
        "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
        "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise, (n_samples, y_dim))\n",
        "\n",
        "# Let's see how these x and y samples look like\n",
        "print('x shape: ', x_samples.shape, '; y shape: ', y_samples.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbE9jyEDzMuJ",
        "outputId": "5c5ca38e-c016-47f9-aa9b-5c82615c8c69"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape:  (20, 10) ; y shape:  (20, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have initialized the inputs and the parameters, let's define the loss function and set up the gradient descent"
      ],
      "metadata": {
        "id": "B4uwzEVB0rWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def mse(params, x_batched, y_batched):\n",
        "  # Define the squared loss for a single pair (x,y)\n",
        "  def squared_error(x, y):\n",
        "    pred = model.apply(params, x)\n",
        "    return jnp.inner(y - pred, y - pred) / 2.0\n",
        "\n",
        "  # Vectorize the previous to compute the average of the loss on all samples\n",
        "  # For the uninitiated, jax.vmap takes the vector of x samples and y samples,\n",
        "  # applies them to the function squared_error\n",
        "  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)"
      ],
      "metadata": {
        "id": "jkrngmhI1u9O"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now applying Gradient Descent\n",
        "learning_rate = 0.3  # Gradient step size\n",
        "print('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\n",
        "\n",
        "# Using the magic wand here - jax.value_and_grad which computes the value\n",
        "# and gradient on the linear regression model\n",
        "loss_grad_fn = jax.value_and_grad(mse)\n",
        "\n",
        "@jax.jit\n",
        "def update_params(params, learning_rate, grads):\n",
        "  params = jax.tree_util.tree_map(\n",
        "      lambda p, g: p - learning_rate * g, params, grads)\n",
        "  return params\n",
        "\n",
        "for i in range(101):\n",
        "  # Perform one gradient update.\n",
        "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
        "  params = update_params(params, learning_rate, grads)\n",
        "  if i % 10 == 0:\n",
        "    print(f'Loss step {i}: ', loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7f_hPVj2eFE",
        "outputId": "24e8de8c-db1a-406e-8b57-ee6c603d9d67"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for \"true\" W,b:  0.023639793\n",
            "Loss step 0:  35.343876\n",
            "Loss step 10:  0.514347\n",
            "Loss step 20:  0.11384159\n",
            "Loss step 30:  0.039326735\n",
            "Loss step 40:  0.019916208\n",
            "Loss step 50:  0.014209136\n",
            "Loss step 60:  0.012425654\n",
            "Loss step 70:  0.01185039\n",
            "Loss step 80:  0.011661784\n",
            "Loss step 90:  0.011599409\n",
            "Loss step 100:  0.011578695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ayqkYb0YOu88"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}