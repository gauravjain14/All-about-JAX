{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flax Basics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNHajXjkgdJ0tOcM3CNDCYc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravjain14/All-about-JAX/blob/main/Flax_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade -q pip jax jaxlib \n",
        "!pip install --upgrade git+https://github.com/google/flax.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auHDS0OkneXE",
        "outputId": "1b7e01c5-4c31-4d66-e78c-7d06ea6bc630"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.0 MB 4.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 25.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 72.0 MB 1.3 MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/google/flax.git\n",
            "  Cloning https://github.com/google/flax.git to /tmp/pip-req-build-flhgtbd2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/google/flax.git /tmp/pip-req-build-flhgtbd2\n",
            "  Resolved https://github.com/google/flax.git to commit fb253d05f3ae93942c407f1131a816e225b179ae\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (1.21.6)\n",
            "Requirement already satisfied: jax>=0.3.14 in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (0.3.15)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (1.0.4)\n",
            "Collecting optax\n",
            "  Downloading optax-0.1.3-py3-none-any.whl (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.1/145.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rich~=11.1\n",
            "  Downloading rich-11.2.0-py3-none-any.whl (217 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.3/217.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from flax==0.5.3) (4.1.1)\n",
            "Collecting PyYAML>=5.4.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.14->flax==0.5.3) (3.3.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.14->flax==0.5.3) (0.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.14->flax==0.5.3) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.14->flax==0.5.3) (1.7.3)\n",
            "Collecting commonmark<0.10.0,>=0.9.0\n",
            "  Downloading commonmark-0.9.1-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich~=11.1->flax==0.5.3) (2.6.1)\n",
            "Collecting colorama<0.5.0,>=0.4.0\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.5.3) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.5.3) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.5.3) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax==0.5.3) (0.11.0)\n",
            "Collecting chex>=0.0.4\n",
            "  Downloading chex-0.1.3-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.2/72.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax->flax==0.5.3) (0.3.15)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax==0.5.3) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax==0.5.3) (0.1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->flax==0.5.3) (1.15.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.3.14->flax==0.5.3) (5.9.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.3.14->flax==0.5.3) (3.8.1)\n",
            "Building wheels for collected packages: flax\n",
            "  Building wheel for flax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flax: filename=flax-0.5.3-py3-none-any.whl size=202200 sha256=7f57e7f053fafa41f8e9e6459de27b4fe491c372dba14e3bcf07421f690c96d6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8ludvtkf/wheels/3e/9e/df/f792f5b2c0d0ccf00f3c1286bc060dab7cb2a4af5134d1c5b4\n",
            "Successfully built flax\n",
            "Installing collected packages: commonmark, PyYAML, colorama, rich, chex, optax, flax\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 chex-0.1.3 colorama-0.4.5 commonmark-0.9.1 flax-0.5.3 optax-0.1.3 rich-11.2.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YQG3pqpPeVPL"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "from typing import Any, Callable, Sequence\n",
        "from jax import lax, random, numpy as jnp\n",
        "from flax.core import freeze, unfreeze\n",
        "from flax import linen as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression with Flax "
      ],
      "metadata": {
        "id": "pYIvTWQQnT_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create one dense layer instance\n",
        "# We only specify the output size of the model. Size of the \n",
        "# input is idenfied by the correct size of the kerne\n",
        "model = nn.Dense(features=5) # Number of 'features' parameter as input "
      ],
      "metadata": {
        "id": "pJ2-Tn2hqRgW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Follow this up with Model parameters and Initialization"
      ],
      "metadata": {
        "id": "33wFesCGqnDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "key1, key2 = random.split(random.PRNGKey(0))\n",
        "x = random.normal(key1, (10,))  # generate dummy input\n",
        "params = model.init(key2, x)\n",
        "jax.tree_util.tree_map(lambda x: x.shape, params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EKETZ0Rq7F7",
        "outputId": "5b88332a-1b5e-4be0-c53f-6f35ff26b396"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    params: {\n",
              "        bias: (5,),\n",
              "        kernel: (10, 5),\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The params created are immutable i.e. retains the functional nature of JAX\n",
        "try:\n",
        "  params['new_key'] = jnp.ones((2,2))\n",
        "except ValueError as e:\n",
        "  print(\"Error: \", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am31haIYrWOV",
        "outputId": "e0e3b482-bbf2-4724-a6dc-39d989e220f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error:  FrozenDict is immutable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How do we evaluate a model given a set of parameters?\n",
        "\n",
        "We execute `model.apply(parameters, input)`\n",
        "\n",
        "Note: Seems like when we call the `print` method on the model output, it copies that to the host."
      ],
      "metadata": {
        "id": "2tVEV9CZtyZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = model.apply(params, x)\n",
        "model_output\n",
        "# print(model_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI9_N9TYuQ3G",
        "outputId": "3124ee0e-f107-4dc2-da4d-217941b293af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeviceArray([-1.3721193 ,  0.61131495,  0.6442836 ,  2.2192965 ,\n",
              "             -1.1271116 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Mf0VbsuauZ7k"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Now that we know how to initialize parameters and apply them to a model, let's learn how to use the same \"immutable\" parameters and execute **Gradient Descent**. \n",
        "\n",
        "Confused about **Gradient Descent**, feel free to take a look at [this link](https://flax.readthedocs.io/en/latest/guides/flax_basics.html#gradient-descent)"
      ],
      "metadata": {
        "id": "QEUHF9CBwtDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's begin with setting up initial probelm dimensions\n",
        "n_samples = 20\n",
        "x_dim = 10\n",
        "y_dim = 5\n",
        "\n",
        "## Initialize parameters (W) and bias (b)\n",
        "key = random.PRNGKey(0)\n",
        "k1, k2 = random.split(key)\n",
        "W = random.normal(k1, (x_dim, y_dim))\n",
        "b = random.normal(k2, (y_dim,))\n",
        "\n",
        "# This is a first - store the parameters in a pytree\n",
        "true_params = freeze({'params': {'bias':b, 'kernel': W}})\n",
        "# We can look at how this pytree looks like\n",
        "true_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK3LJVIXxKgH",
        "outputId": "b25ae2c2-cf3d-4765-86ec-39c47e6fb8a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FrozenDict({\n",
              "    params: {\n",
              "        bias: DeviceArray([-1.4581939, -2.047044 ,  2.0473392,  1.1684095, -0.9758364],            dtype=float32),\n",
              "        kernel: DeviceArray([[ 1.0247566 ,  0.18528093,  0.03387944, -0.86629736,\n",
              "                       0.34718114],\n",
              "                     [ 1.7656006 ,  0.99169755,  1.1657897 ,  1.1106981 ,\n",
              "                      -0.08589564],\n",
              "                     [-1.1820309 ,  0.29050717,  1.436301  ,  0.15073189,\n",
              "                      -1.3651401 ],\n",
              "                     [-1.1463748 , -0.16064964,  0.04578291,  1.3267074 ,\n",
              "                       0.08830649],\n",
              "                     [ 0.15840754,  1.3908992 , -1.3764939 ,  0.4419787 ,\n",
              "                      -2.2242246 ],\n",
              "                     [ 0.5943986 ,  0.8191525 ,  0.32800463,  0.51409715,\n",
              "                       0.92392564],\n",
              "                     [-0.32272884,  1.7835051 ,  1.0902369 , -0.5799917 ,\n",
              "                       0.9487662 ],\n",
              "                     [ 0.97157586, -1.2998172 ,  0.3205269 ,  0.806568  ,\n",
              "                      -1.1939563 ],\n",
              "                     [ 1.001932  , -0.6774013 ,  1.0407888 , -1.8285896 ,\n",
              "                      -0.4360311 ],\n",
              "                     [-0.62268263,  0.47034723, -1.1581832 , -0.71849173,\n",
              "                       0.13199319]], dtype=float32),\n",
              "    },\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate input samples with additional noise\n",
        "key_sample, key_noise = random.split(k1)\n",
        "x_samples = random.normal(key_sample, (n_samples, x_dim))\n",
        "y_samples = jnp.dot(x_samples, W) + b + 0.1 * random.normal(key_noise, (n_samples, y_dim))\n",
        "\n",
        "# Let's see how these x and y samples look like\n",
        "print('x shape: ', x_samples.shape, '; y shape: ', y_samples.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbE9jyEDzMuJ",
        "outputId": "a4b74a15-3328-4f56-8b49-95072496d94f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x shape:  (20, 10) ; y shape:  (20, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have initialized the inputs and the parameters, let's define the loss function and set up the gradient descent"
      ],
      "metadata": {
        "id": "B4uwzEVB0rWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def mse(params, x_batched, y_batched):\n",
        "  # Define the squared loss for a single pair (x,y)\n",
        "  def squared_error(x, y):\n",
        "    pred = model.apply(params, x)\n",
        "    return jnp.inner(y - pred, y - pred) / 2.0\n",
        "\n",
        "  # Vectorize the previous to compute the average of the loss on all samples\n",
        "  # For the uninitiated, jax.vmap takes the vector of x samples and y samples,\n",
        "  # applies them to the function squared_error\n",
        "  return jnp.mean(jax.vmap(squared_error)(x_batched, y_batched), axis=0)"
      ],
      "metadata": {
        "id": "jkrngmhI1u9O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Now applying Gradient Descent\n",
        "learning_rate = 0.3  # Gradient step size\n",
        "print('Loss for \"true\" W,b: ', mse(true_params, x_samples, y_samples))\n",
        "\n",
        "# Using the magic wand here - jax.value_and_grad which computes the value\n",
        "# and gradient on the linear regression model\n",
        "loss_grad_fn = jax.value_and_grad(mse)\n",
        "\n",
        "@jax.jit\n",
        "def update_params(params, learning_rate, grads):\n",
        "  params = jax.tree_util.tree_map(\n",
        "      lambda p, g: p - learning_rate * g, params, grads)\n",
        "  return params\n",
        "\n",
        "for i in range(101):\n",
        "  # Perform one gradient update.\n",
        "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
        "  params = update_params(params, learning_rate, grads)\n",
        "  if i % 10 == 0:\n",
        "    print(f'Loss step {i}: ', loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7f_hPVj2eFE",
        "outputId": "8398bf6b-4a43-494f-843d-ab511d20cc38"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for \"true\" W,b:  0.023639793\n",
            "Loss step 0:  35.343876\n",
            "Loss step 10:  0.514347\n",
            "Loss step 20:  0.11384159\n",
            "Loss step 30:  0.039326735\n",
            "Loss step 40:  0.019916208\n",
            "Loss step 50:  0.014209136\n",
            "Loss step 60:  0.012425654\n",
            "Loss step 70:  0.01185039\n",
            "Loss step 80:  0.011661784\n",
            "Loss step 90:  0.011599409\n",
            "Loss step 100:  0.011578695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ayqkYb0YOu88"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizing with Optax\n",
        "\n",
        "Optax is an optimization package for doing stuff like gradient transformations, changing hyperparameters over time, applying updates to different parts of the parameter tree, and more."
      ],
      "metadata": {
        "id": "pYki-KYN8bcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import optax\n",
        "tx = optax.sgd(learning_rate=learning_rate)\n",
        "opt_state = tx.init(params)\n",
        "loss_grad_fn = jax.value_and_grad(mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25Sk8Wp38v3b",
        "outputId": "e9da8ad8-b57d-49fa-82e0-6a0ecc783dc1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
            "  PyTreeDef = type(jax.tree_structure(None))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(101):\n",
        "  loss_val, grads = loss_grad_fn(params, x_samples, y_samples)\n",
        "  updates, opt_state = tx.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  if i % 10 == 0:\n",
        "    print('Loss step {}: '.format(i), loss_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNg8SaVr87Dh",
        "outputId": "06ebe7a3-9e04-4351-ff18-83d9a63042b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss step 0:  0.011577628\n",
            "Loss step 10:  0.011571462\n",
            "Loss step 20:  0.011569391\n",
            "Loss step 30:  0.011568717\n",
            "Loss step 40:  0.011568484\n",
            "Loss step 50:  0.011568407\n",
            "Loss step 60:  0.01156839\n",
            "Loss step 70:  0.0115683675\n",
            "Loss step 80:  0.011568379\n",
            "Loss step 90:  0.011568378\n",
            "Loss step 100:  0.011568374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IF24g4Ju-plP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's define our own models"
      ],
      "metadata": {
        "id": "KsJ0zdBJAe3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExplicitMLP(nn.Module):\n",
        "  features: Sequence[int]\n",
        "\n",
        "  def setup(self):\n",
        "    \"\"\"In setup, you are able to name some sublayers and keep them around\n",
        "     for further use (e.g. encoder/decoder methods in autoencoders). \"\"\"\n",
        "    self.layers = [nn.Dense(feat) for feat in self.features]\n",
        "  \n",
        "  def __call__(self, inputs):\n",
        "    x = inputs\n",
        "    for i, lyr in enumerate(self.layers):\n",
        "      x = lyr(x)\n",
        "      if i != len(self.layers) - 1:\n",
        "        x = nn.relu(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "j6PxOccaAilV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's initialize and apply params to the model\n",
        "\n",
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "# Input is defined as a tensor of shape [4, 4]\n",
        "x = random.uniform(key1, (4,4))\n",
        "# Initializes the multi-layer MLP;\n",
        "# Each shape corresponds to the size of the output of the layer\n",
        "model = ExplicitMLP(features=[3,4,5])\n",
        "\n",
        "# This is important because this is where the JIT feature kicks in and the\n",
        "# model is lazily initialized with param shapes but without actually running\n",
        "# the inputs\n",
        "# model.init and model.apply, both call the __call__ function\n",
        "params = model.init(key2, x)\n",
        "\n",
        "# Because layers are not defined in the ExplicitMLP, we can't directly call\n",
        "# model(x); \n",
        "y = model.apply(params, x)\n",
        "\n",
        "print('initialized parameter shapes: \\n', jax.tree_util.tree_map(jnp.shape, \n",
        "                                                            unfreeze(params)))\n",
        "print('output: \\n', y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYOPu4BrCSNn",
        "outputId": "93092cf1-e6d7-43e4-a8f1-8689c0461ba8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initialized parameter shapes: \n",
            " {'params': {'layers_0': {'bias': (3,), 'kernel': (4, 3)}, 'layers_1': {'bias': (4,), 'kernel': (3, 4)}, 'layers_2': {'bias': (5,), 'kernel': (4, 5)}}}\n",
            "output: \n",
            " [[ 0.          0.          0.          0.          0.        ]\n",
            " [ 0.0072379  -0.00810348 -0.0255094   0.02151717 -0.01261241]\n",
            " [ 0.          0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A segue to underlying APIs\n",
        "\n",
        "Looking at the Model setup in [Section Let's define our own models](https://colab.research.google.com/drive/1_2fZs7dQsAZwFBoKAJd4SpnrRz0gQd1L?authuser=1#scrollTo=Let_s_define_our_own_models), it occurred to me there are a lot of APIs, like the `lax.dot_general` that don't really make sense.\n",
        "\n",
        "So, this segue is to explore them using quick examples"
      ],
      "metadata": {
        "id": "aY_8xEp4m14I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A sample kernel of shape [3, 4]\n",
        "sample_kernel = jnp.array([[1, 2, 3, 0], [4, 5, 6, 0], [7, 8, 9, 0]])\n",
        "\n",
        "# A sample input of shape [3, 3]\n",
        "sample_inp = jnp.array([[1, 1, 1], [1, 1, 1], [1, 1, 1]])\n",
        "\n",
        "# Let's look at what lax.dot_general is all about;\n",
        "# It takes a dot product. However, it is slightly complicated because\n",
        "# the API requires us to provide the dimensions along which reduction\n",
        "# should be performed. \n",
        "# Also, we need to specify the batch dimension i.e. which dimension of each\n",
        "# tensor should be used as a Batch. It can be left empty\n",
        "print(lax.dot_general(sample_inp, sample_kernel, (((1,), (0,)), ((), ())),))\n",
        "\n",
        "# Here, we eliminate the reduction dimension (it takes default to be the\n",
        "# innermost of the lhs and outermost of the rhs) and introduce a batch\n",
        "# dimension. \n",
        "# An important thing to note for the batch dimension number is that the number\n",
        "# of batches should be the same.\n",
        "\n",
        "print(lax.dot_general(sample_inp, sample_kernel, (((), ()), ((0,), (0,))),))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOTJ6HSPCt0c",
        "outputId": "f28ea6d1-be40-49d1-f941-3a3b34ffd188"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[12 15 18  0]\n",
            " [12 15 18  0]\n",
            " [12 15 18  0]]\n",
            "[[[1 2 3 0]\n",
            "  [1 2 3 0]\n",
            "  [1 2 3 0]]\n",
            "\n",
            " [[4 5 6 0]\n",
            "  [4 5 6 0]\n",
            "  [4 5 6 0]]\n",
            "\n",
            " [[7 8 9 0]\n",
            "  [7 8 9 0]\n",
            "  [7 8 9 0]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting back to working with MLPs and our own custom Model"
      ],
      "metadata": {
        "id": "XiTDL7tzN4vQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module parameters\n",
        "\n",
        "Defining our own Dense layer"
      ],
      "metadata": {
        "id": "N4lfXcWgpZ0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDense(nn.Module):\n",
        "  features: int\n",
        "  # Using typing.Callable allows any number of arguments to the function and\n",
        "  # allow the function to reurn Any \n",
        "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    \"\"\" __call__ is instantiated when model.init is called and by default\n",
        "    model.apply jumps to the __call__ method.\n",
        "\n",
        "    The \"method\" argument in apply() should be set if the user intends to call\n",
        "    \"apply\" on a different method than __call__.\n",
        "    \"\"\"\n",
        "    kernel = self.param('kernel',\n",
        "                        self.kernel_init,\n",
        "                        (inputs.shape[-1], self.features))\n",
        "    y = lax.dot_general(inputs, kernel,\n",
        "                        (((inputs.ndim - 1,), (0,)), ((), ())),)\n",
        "    bias = self.param('bias', self.bias_init, (self.features,))\n",
        "    y = y + bias\n",
        "    return y"
      ],
      "metadata": {
        "id": "XyAMqs-nqZUs"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "x = random.uniform(key1, (4,4))\n",
        "print('Initializing the model')\n",
        "model = SimpleDense(features=3)\n",
        "print('Initializing the Parameters')\n",
        "params = model.init(key2, x)\n",
        "y = model.apply(params, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIIIoTayukYl",
        "outputId": "ae0dcaf7-1cd2-4a45-a52c-c112ee8919cb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing the model\n",
            "Initializing the Parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDenseMadeComplicated(nn.Module):\n",
        "  features: int\n",
        "  # Using typing.Callable allows any number of arguments to the function and\n",
        "  # allow the function to reurn Any \n",
        "  kernel_init: Callable = nn.initializers.lecun_normal()\n",
        "  bias_init: Callable = nn.initializers.zeros\n",
        "\n",
        "  def setup(self):\n",
        "    \"\"\" \n",
        "    setup() doesn't accept any inputs. \n",
        "    Only initialize the kernel and bias in the __call__ method.\n",
        "    We'll pass a \"method\" argument to apply()\n",
        "    \"\"\"\n",
        "    self.intermediate_layers = [nn.Dense(feature) for feature in self.features[1:]]\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, inputs):\n",
        "    kernel = self.param('kernel',\n",
        "                        self.kernel_init,\n",
        "                        (inputs.shape[-1], self.features[0]))\n",
        "    # Apply bias only on the first layer\n",
        "    bias = self.param('bias', self.bias_init, (self.features[0],))\n",
        "    x = lax.dot_general(inputs, kernel,\n",
        "                        (((inputs.ndim - 1,), (0,)), ((), ())),)\n",
        "    x += bias\n",
        "    for i, lyr in enumerate(self.intermediate_layers):\n",
        "      x = lyr(x)\n",
        "      if i != len(self.intermediate_layers) - 1:\n",
        "        x = nn.relu(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ByqK8QLovkHt"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key1, key2 = random.split(random.PRNGKey(0), 2)\n",
        "x = random.uniform(key1, (4,4))\n",
        "print('Initializing the model')\n",
        "model = SimpleDenseMadeComplicated(features=[3, 4, 5])\n",
        "print('Initializing the Parameters')\n",
        "params = model.init(key2, x)\n",
        "print(params)\n",
        "y = model.apply(params, x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4sNbI9JejnN",
        "outputId": "f3f24e32-418c-4d74-a262-ae714e76f42e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing the model\n",
            "Initializing the Parameters\n",
            "FrozenDict({\n",
            "    params: {\n",
            "        kernel: DeviceArray([[ 0.61506   , -0.22728713,  0.6054702 ],\n",
            "                     [-0.29617992,  1.1232013 , -0.879759  ],\n",
            "                     [-0.35162622,  0.3806491 ,  0.6893246 ],\n",
            "                     [-0.1151355 ,  0.04567898, -1.091212  ]], dtype=float32),\n",
            "        bias: DeviceArray([0., 0., 0.], dtype=float32),\n",
            "        intermediate_layers_0: {\n",
            "            kernel: DeviceArray([[ 0.7151457 ,  0.12737666, -1.0549262 ,  0.08998597],\n",
            "                         [-0.74000335,  1.1685069 , -0.26688337, -0.24311556],\n",
            "                         [-0.20404899,  0.4641534 ,  0.03094014,  0.26043552]],            dtype=float32),\n",
            "            bias: DeviceArray([0., 0., 0., 0.], dtype=float32),\n",
            "        },\n",
            "        intermediate_layers_1: {\n",
            "            kernel: DeviceArray([[ 0.38301077, -0.21687   , -0.44121116,  0.10675155,\n",
            "                           0.41591075],\n",
            "                         [-1.0800076 ,  0.32631534, -0.21117473, -0.5597935 ,\n",
            "                           0.3258297 ],\n",
            "                         [-0.83598965, -0.04183271, -0.65744895, -0.03614644,\n",
            "                           0.01123837],\n",
            "                         [-0.60829896, -0.19334063,  0.30166268,  0.8152896 ,\n",
            "                          -0.4290769 ]], dtype=float32),\n",
            "            bias: DeviceArray([0., 0., 0., 0., 0.], dtype=float32),\n",
            "        },\n",
            "    },\n",
            "})\n",
            "[[-1.0528371   0.31810603 -0.20586208 -0.5457104   0.31763262]\n",
            " [-0.6054499   0.14094085 -0.18883765 -0.25717422  0.14829133]\n",
            " [-0.87272793  0.26368752 -0.17064518 -0.45235556  0.2632951 ]\n",
            " [-0.50096637  0.15136285 -0.09795436 -0.25966275  0.15113759]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UpwZu597epxN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}